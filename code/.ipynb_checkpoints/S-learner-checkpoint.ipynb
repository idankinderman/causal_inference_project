{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7812c343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\idan\\anaconda3\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\idan\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\idan\\anaconda3\\lib\\site-packages (from xgboost) (1.11.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f328fb36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T20:17:44.166232Z",
     "start_time": "2024-09-15T20:17:44.028410Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9f0667",
   "metadata": {},
   "source": [
    "# S-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "18358cfe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T20:17:45.860981Z",
     "start_time": "2024-09-15T20:17:45.133736Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor  # Import MLPRegressor\n",
    "\n",
    "class S_learner:\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, T_train, T_test, train_on_full_data=False):\n",
    "        # Validate data before initializing\n",
    "        inputs = [x_train, y_train, x_test, y_test, T_train, T_test]\n",
    "        for dataset in inputs:\n",
    "            self.check_nulls_in_dataframe(dataset)\n",
    "        \n",
    "        self.x_train = x_train\n",
    "        self.y_train = y_train\n",
    "        self.x_test = x_test\n",
    "        self.y_test = y_test\n",
    "        self.T_train = T_train\n",
    "        self.T_test = T_test\n",
    "        self.train_on_full_data = train_on_full_data\n",
    "        self.models = {}\n",
    "        self.measures = {}\n",
    "\n",
    "    def fit(self, x_train=None, T_train=None, y_train=None, bootstrap=False):\n",
    "        if x_train is None or T_train is None or y_train is None:\n",
    "            x_train = self.x_train\n",
    "            T_train = self.T_train\n",
    "            y_train = self.y_train\n",
    "            \n",
    "        x_train_full = x_train.copy()\n",
    "        x_train_full.loc[:, \"T\"] = T_train\n",
    "        y_train = y_train.copy()\n",
    "        \n",
    "        if self.train_on_full_data:\n",
    "            x_test_full = self.x_test.copy()\n",
    "            x_test_full.loc[:, \"T\"] = self.T_test\n",
    "            x_train_full = pd.concat([x_train_full, x_test_full], axis=0)\n",
    "            y_test = self.y_test.copy()\n",
    "            y_train = pd.concat([y_train, y_test], axis=0)\n",
    "        \n",
    "        if not bootstrap:\n",
    "            self.models['linear_regression'] = LinearRegression()\n",
    "            self.models['linear_regression'].fit(x_train_full, y_train)\n",
    "            print(\"Fitted Linear Regression\")\n",
    "\n",
    "        self.models['svr_rbf'] = SVR(kernel='rbf')\n",
    "        self.models['svr_rbf'].fit(x_train_full, y_train)\n",
    "        if not bootstrap:\n",
    "            print(\"Fitted SVR with RBF kernel\")\n",
    "        \n",
    "        if not bootstrap:\n",
    "            self.models['svr_poly'] = SVR(kernel='poly', degree=2)\n",
    "            self.models['svr_poly'].fit(x_train_full, y_train)\n",
    "            print(\"Fitted SVR with Polynomial kernel\")\n",
    "\n",
    "        self.models['gradient_boosting'] = GradientBoostingRegressor()\n",
    "        self.models['gradient_boosting'].fit(x_train_full, y_train)\n",
    "        if not bootstrap:\n",
    "            print(\"Fitted Gradient Boosting Regressor\")\n",
    "\n",
    "        self.models['random_forest'] = RandomForestRegressor()\n",
    "        self.models['random_forest'].fit(x_train_full, y_train)\n",
    "        if not bootstrap:\n",
    "            print(\"Fitted RandomForest Regressor\")\n",
    "        \n",
    "        if not bootstrap:\n",
    "            self.models['mlp'] = MLPRegressor(hidden_layer_sizes=(192,), max_iter=1500, activation='relu', \n",
    "                                              solver='adam', learning_rate_init=0.00005, random_state=42)\n",
    "            self.models['mlp'].fit(x_train_full, y_train)\n",
    "            print(\"Fitted MLP Regressor\")\n",
    "        \n",
    "\n",
    "    def evaluate(self):\n",
    "        x_train_full = self.x_train.copy()\n",
    "        x_test_full = self.x_test.copy()\n",
    "        \n",
    "        x_train_full.loc[:, \"T\"] = self.T_train\n",
    "        x_test_full.loc[:, \"T\"] = self.T_test\n",
    "        \n",
    "        results = {}\n",
    "        for name, model in self.models.items():\n",
    "            train_pred = model.predict(x_train_full)\n",
    "            test_pred = model.predict(x_test_full)\n",
    "            train_mse = mean_squared_error(self.y_train, train_pred)\n",
    "            test_mse = mean_squared_error(self.y_test, test_pred)\n",
    "            results[name] = {'Train MSE': train_mse, 'Test MSE': test_mse}\n",
    "        \n",
    "        for name, scores in results.items():\n",
    "            print(f\"{name}: Train MSE = {scores['Train MSE']:.4f}, Test MSE = {scores['Test MSE']:.4f}\")\n",
    "    \n",
    "    def compute_ATE(self, bootstrap=False):\n",
    "        x_combined = pd.concat([self.x_train, self.x_test], axis=0)\n",
    "        self.compute_effect(x=x_combined, measure_name='ATE', bootstrap=bootstrap)\n",
    "    \n",
    "    def compute_ATE_final(self, list_of_models, bootstrap=False):\n",
    "        final_ATE = 0\n",
    "        for model in list_of_models:\n",
    "            final_ATE += self.measures['ATE'][model]\n",
    "        final_ATE = final_ATE / len(list_of_models)\n",
    "        if not bootstrap:\n",
    "            print(\"\\nFinal ATE:\", final_ATE)\n",
    "        return final_ATE\n",
    "        \n",
    "    def compute_effect(self, x, measure_name, bootstrap=False):\n",
    "        self.measures[measure_name] = {}\n",
    "        \n",
    "        x_with_T_zero = x.copy()\n",
    "        x_with_T_zero['T'] = 0\n",
    "\n",
    "        x_with_T_one = x.copy()\n",
    "        x_with_T_one['T'] = 1\n",
    "        \n",
    "        for key in self.models:\n",
    "            predictions_one = self.models[key].predict(x_with_T_one)\n",
    "            predictions_zero = self.models[key].predict(x_with_T_zero)\n",
    "            diffrences = predictions_one - predictions_zero\n",
    "            self.measures[measure_name][key] = np.mean(diffrences)\n",
    "        \n",
    "        if not bootstrap:\n",
    "            print()\n",
    "            print(f\"The {measure_name} are \", self.measures[measure_name])\n",
    "            print()\n",
    "    \n",
    "    def bootstrap(self, num_trials=200, alpha=0.05):\n",
    "        n = len(self.x_train)\n",
    "        self.ATE_list = []  # To store ATEs from each bootstrap sample\n",
    "\n",
    "        for i in range(num_trials):\n",
    "            if i in [num_trials//50, num_trials//5, 2*num_trials//5, 3*num_trials//5, 4*num_trials//5]:\n",
    "                print(f\"bootstrap {i}/{num_trials}\")\n",
    "            \n",
    "            # Step 1: Generate bootstrap sample indices\n",
    "            bootstrap_indices = np.random.choice(n, size=n, replace=True)\n",
    "\n",
    "            # Step 2: Create bootstrap samples\n",
    "            x_train_boot = self.x_train.iloc[bootstrap_indices]\n",
    "            y_train_boot = self.y_train[bootstrap_indices]\n",
    "            T_train_boot = self.T_train[bootstrap_indices]\n",
    "\n",
    "            # Step 3: Train the model on bootstrap sample\n",
    "            self.fit(x_train=x_train_boot, T_train=T_train_boot, y_train=y_train_boot, bootstrap=True)\n",
    "\n",
    "            # Step 4: Compute ATE\n",
    "            self.compute_ATE(bootstrap=True)\n",
    "            final_ATE = self.compute_ATE_final(['svr_rbf', 'gradient_boosting', 'random_forest'], bootstrap=True)\n",
    "\n",
    "            # Assume that compute_ATE_final updates self.final_ATE\n",
    "            # Store the final ATE\n",
    "            self.ATE_list.append(final_ATE)\n",
    "\n",
    "        # Convert ATE list to a NumPy array for numerical operations\n",
    "        ATE_array = np.array(self.ATE_list)\n",
    "\n",
    "        # Step 5: Calculate mean ATE\n",
    "        mean_ATE = np.mean(ATE_array)\n",
    "\n",
    "        # Step 6: Calculate confidence intervals\n",
    "        lower_percentile = (alpha / 2) * 100\n",
    "        upper_percentile = (1 - alpha / 2) * 100\n",
    "        CI_lower = np.percentile(ATE_array, lower_percentile)\n",
    "        CI_upper = np.percentile(ATE_array, upper_percentile)\n",
    "\n",
    "        # Step 7: Return results\n",
    "        print(\"mean_ATE\", mean_ATE, \"CI_lower\", CI_lower, \"CI_upper\", CI_upper)\n",
    "        return mean_ATE, CI_lower, CI_upper\n",
    "               \n",
    "    \n",
    "    def check_nulls_in_dataframe(self, df):\n",
    "        # Check if there are any null values in the DataFrame\n",
    "        if df.isnull().any().any():\n",
    "            print(\"The DataFrame contains null values.\")\n",
    "            # Count of nulls in each column\n",
    "            null_counts = df.isnull().sum()\n",
    "            print(\"Count of null values in each column:\")\n",
    "            print(null_counts[null_counts > 0])\n",
    "        else:\n",
    "            print(\"The DataFrame does not contain any null values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9806a4a",
   "metadata": {},
   "source": [
    "# T-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4eb6ea3c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T20:17:45.949599Z",
     "start_time": "2024-09-15T20:17:45.926756Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.neural_network import MLPRegressor  # Import MLPRegressor\n",
    "\n",
    "class T_learner:\n",
    "    def __init__(self, x_train, y_train, x_test, y_test, T_train, T_test, train_on_full_data=False):\n",
    "        # Validate data before initializing\n",
    "        inputs = [x_train, y_train, x_test, y_test, T_train, T_test]\n",
    "        for dataset in inputs:\n",
    "            self.check_nulls_in_dataframe(dataset)\n",
    "        \n",
    "        treated_indices_train = T_train == 1\n",
    "        treated_indices_test = T_test == 1\n",
    "        \n",
    "        # x\n",
    "        self.x = {'train': {}, 'test': {}}\n",
    "        self.x['train'][0] = x_train[~treated_indices_train]\n",
    "        self.x['train'][1] = x_train[treated_indices_train]\n",
    "        self.x['test'][0] = x_test[~treated_indices_test]\n",
    "        self.x['test'][1] = x_test[treated_indices_test]\n",
    "        \n",
    "        # y\n",
    "        self.y = {'train': {}, 'test': {}}\n",
    "        self.y['train'][0] = y_train[~treated_indices_train]\n",
    "        self.y['train'][1] = y_train[treated_indices_train]\n",
    "        self.y['test'][0] = y_test[~treated_indices_test]\n",
    "        self.y['test'][1] = y_test[treated_indices_test]\n",
    "        \n",
    "        # else\n",
    "        self.train_on_full_data = train_on_full_data\n",
    "        self.models = {0:{}, 1:{}}\n",
    "        self.measures = {}\n",
    "        \n",
    "\n",
    "    def fit(self, x_train=None, y_train=None, bootstrap=False):\n",
    "        if x_train is None or y_train is None:\n",
    "            x_train = self.x_train\n",
    "            y_train = self.y_train\n",
    "            \n",
    "        for T in [0,1]:\n",
    "            if with_prints:\n",
    "                print(f\"\\nFitting models with T = {T}\")\n",
    "            \n",
    "            x_train_full = x['train'][T].copy()\n",
    "            y_train = y['train'][T].copy()\n",
    "\n",
    "            if self.train_on_full_data:\n",
    "                x_test_full = self.x['test'][T].copy()\n",
    "                x_train_full = pd.concat([x_train_full, x_test_full], axis=0)\n",
    "                y_test = self.y['test'][T].copy()\n",
    "                y_train = pd.concat([y_train, y_test], axis=0)\n",
    "            \n",
    "            if not bootstrap:\n",
    "                self.models[T]['linear_regression'] = LinearRegression()\n",
    "                self.models[T]['linear_regression'].fit(x_train_full, y_train)\n",
    "                print(\"Fitted Linear Regression\")\n",
    "\n",
    "            self.models[T]['svr_rbf'] = SVR(kernel='rbf')\n",
    "            self.models[T]['svr_rbf'].fit(x_train_full, y_train)\n",
    "            if not bootstrap:\n",
    "                print(\"Fitted SVR with RBF kernel\")\n",
    "            \n",
    "            if not bootstrap:\n",
    "                self.models[T]['svr_poly'] = SVR(kernel='poly', degree=2)\n",
    "                self.models[T]['svr_poly'].fit(x_train_full, y_train)\n",
    "                print(\"Fitted SVR with Polynomial kernel\")\n",
    "\n",
    "            self.models[T]['gradient_boosting'] = GradientBoostingRegressor()\n",
    "            self.models[T]['gradient_boosting'].fit(x_train_full, y_train)\n",
    "            if not bootstrap:\n",
    "                print(\"Fitted Gradient Boosting Regressor\")\n",
    "\n",
    "            self.models[T]['random_forest'] = RandomForestRegressor()\n",
    "            self.models[T]['random_forest'].fit(x_train_full, y_train)\n",
    "            if not bootstrap:\n",
    "                print(\"Fitted RandomForest Regressor\")\n",
    "            \n",
    "            if not bootstrap:\n",
    "                self.models[T]['mlp'] = MLPRegressor(hidden_layer_sizes=(192,), max_iter=1500, activation='relu', \n",
    "                                              solver='adam', learning_rate_init=0.00005, random_state=42)\n",
    "                self.models[T]['mlp'].fit(x_train_full, y_train)\n",
    "                print(\"Fitted MLP Regressor\")\n",
    "            \n",
    "        \n",
    "\n",
    "    def evaluate(self):\n",
    "        results = {0 : {}, 1 : {}}\n",
    "        for T in [0,1]:\n",
    "            x_train_full = self.x['train'][T].copy()\n",
    "            x_test_full = self.x['test'][T].copy()\n",
    "\n",
    "            for name, model in self.models[T].items():\n",
    "                train_pred = model.predict(x_train_full)\n",
    "                test_pred = model.predict(x_test_full)\n",
    "                train_mse = mean_squared_error(self.y['train'][T], train_pred)\n",
    "                test_mse = mean_squared_error(self.y['test'][T], test_pred)\n",
    "                results[T][name] = {'Train MSE': train_mse, 'Test MSE': test_mse}\n",
    "        \n",
    "        for T in [0,1]: \n",
    "            for name, scores in results[T].items():\n",
    "                print(f\"T: {T} | {name}: Train MSE = {scores['Train MSE']:.4f}, Test MSE = {scores['Test MSE']:.4f}\")\n",
    "    \n",
    "    def compute_ATE(self, bootstrap=False):\n",
    "        x_combined = pd.concat([self.x['train'][0], self.x['train'][1], self.x['test'][0], self.x['test'][1]], axis=0)\n",
    "        self.compute_effect(x=x_combined, measure_name='ATE', bootstrap=bootstrap)\n",
    "    \n",
    "    def compute_ATE_final(self, list_of_models, bootstrap=False):\n",
    "        final_ATE = 0\n",
    "        for model in list_of_models:\n",
    "            final_ATE += self.measures['ATE'][model]\n",
    "        final_ATE = final_ATE / len(list_of_models)\n",
    "        if not bootstrap:\n",
    "            print(\"\\nFinal ATE:\", final_ATE)\n",
    "        return final_ATE\n",
    "        \n",
    "    def compute_effect(self, x, measure_name, bootstrap=False):\n",
    "        self.measures[measure_name] = {}\n",
    "        \n",
    "        for key in self.models[0]:\n",
    "            predictions_one = self.models[1][key].predict(x)\n",
    "            predictions_zero = self.models[0][key].predict(x)\n",
    "            diffrences = predictions_one - predictions_zero\n",
    "            #print(f\"{measure_name}, diffrences for {key}:\", diffrences)\n",
    "            self.measures[measure_name][key] = np.mean(diffrences)\n",
    "        \n",
    "        if not bootstrap:\n",
    "            print()\n",
    "            print(f\"The {measure_name} are \", self.measures[measure_name])\n",
    "            print()\n",
    "            \n",
    "            \n",
    "    def bootstrap(self, num_trials=200, alpha=0.05):\n",
    "        n0 = len(self.x['train'][0])\n",
    "        n1 = len(self.x['train'][1])\n",
    "        self.ATE_list = []  # To store ATEs from each bootstrap sample\n",
    "\n",
    "        for i in range(num_trials):\n",
    "            if i in [num_trials//50, num_trials//5, 2*num_trials//5, 3*num_trials//5, 4*num_trials//5]:\n",
    "                print(f\"bootstrap {i}/{num_trials}\")\n",
    "            \n",
    "            # Step 1: Generate bootstrap sample indices\n",
    "            bootstrap_indices0 = np.random.choice(n0, size=n0, replace=True)\n",
    "            bootstrap_indices1 = np.random.choice(n1, size=n1, replace=True)\n",
    "\n",
    "            # Step 2: Create bootstrap samples\n",
    "            x_train_boot = {'train': {'0' : None, '1' : None}}\n",
    "            y_train_boot = {'train': {'0' : None, '1' : None}}\n",
    "            \n",
    "            x_train_boot['train'][0] = self.x['train'][0].iloc[bootstrap_indices0]\n",
    "            x_train_boot['train'][1] = self.x['train'][1].iloc[bootstrap_indices1]\n",
    "            \n",
    "            y_train_boot['train'][0] = self.y['train'][0][bootstrap_indices0]\n",
    "            y_train_boot['train'][1] = self.y['train'][1][bootstrap_indices1]\n",
    "\n",
    "            # Step 3: Train the model on bootstrap sample\n",
    "            self.fit(x_train=x_train_boot, y_train=y_train_boot, bootstrap=True)\n",
    "\n",
    "            # Step 4: Compute ATE\n",
    "            self.compute_ATE(bootstrap=True)\n",
    "            final_ATE = self.compute_ATE_final(['svr_rbf', 'gradient_boosting', 'random_forest'], bootstrap=True)\n",
    "\n",
    "            # Assume that compute_ATE_final updates self.final_ATE\n",
    "            # Store the final ATE\n",
    "            self.ATE_list.append(final_ATE)\n",
    "\n",
    "        # Convert ATE list to a NumPy array for numerical operations\n",
    "        ATE_array = np.array(self.ATE_list)\n",
    "\n",
    "        # Step 5: Calculate mean ATE\n",
    "        mean_ATE = np.mean(ATE_array)\n",
    "\n",
    "        # Step 6: Calculate confidence intervals\n",
    "        lower_percentile = (alpha / 2) * 100\n",
    "        upper_percentile = (1 - alpha / 2) * 100\n",
    "        CI_lower = np.percentile(ATE_array, lower_percentile)\n",
    "        CI_upper = np.percentile(ATE_array, upper_percentile)\n",
    "\n",
    "        # Step 7: Return results\n",
    "        print(\"mean_ATE\", mean_ATE, \"CI_lower\", CI_lower, \"CI_upper\", CI_upper)\n",
    "        return mean_ATE, CI_lower, CI_upper\n",
    "               \n",
    "    \n",
    "    def check_nulls_in_dataframe(self, df):\n",
    "        # Check if there are any null values in the DataFrame\n",
    "        if df.isnull().any().any():\n",
    "            print(\"The DataFrame contains null values.\")\n",
    "            # Count of nulls in each column\n",
    "            null_counts = df.isnull().sum()\n",
    "            print(\"Count of null values in each column:\")\n",
    "            print(null_counts[null_counts > 0])\n",
    "        else:\n",
    "            print(\"The DataFrame does not contain any null values.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a463939a",
   "metadata": {},
   "source": [
    "# Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d302be9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T20:17:46.725661Z",
     "start_time": "2024-09-15T20:17:46.712860Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "class Matching:\n",
    "    def __init__(self, x, y, T):\n",
    "        self.x = x.copy()\n",
    "        self.y = y.copy()\n",
    "        self.T = T.copy()\n",
    "        # Splitting the data based on T\n",
    "        self.x1 = x[T == 1].reset_index(drop=True)\n",
    "        self.y1 = y[T == 1].reset_index(drop=True)\n",
    "        self.x0 = x[T == 0].reset_index(drop=True)\n",
    "        self.y0 = y[T == 0].reset_index(drop=True)\n",
    "        \n",
    "        self.ATE = {}\n",
    "        self.ATT = {}\n",
    "\n",
    "    def compute_ATE(self, k):\n",
    "        # Ensure k is a positive integer\n",
    "        if k <= 0 or not isinstance(k, int):\n",
    "            raise ValueError(\"k must be a positive integer\")\n",
    "        \n",
    "        # Initialize the nearest neighbors models\n",
    "        nn_0 = NearestNeighbors(n_neighbors=k)\n",
    "        nn_1 = NearestNeighbors(n_neighbors=k)\n",
    "        \n",
    "        nn_0.fit(self.x0.values)\n",
    "        nn_1.fit(self.x1.values)\n",
    "        \n",
    "        # Calculate CAT for T=1\n",
    "        CAT_1 = []\n",
    "        for i, xi in self.x1.iterrows():\n",
    "            _, indices = nn_0.kneighbors([xi.values])\n",
    "            y_neighbors = self.y0.iloc[indices[0]]\n",
    "            CAT_1.append(self.y1.loc[i] - y_neighbors.mean())\n",
    "\n",
    "        # Calculate CAT for T=0\n",
    "        CAT_0 = []\n",
    "        for i, xi in self.x0.iterrows():\n",
    "            _, indices = nn_1.kneighbors([xi.values])\n",
    "            y_neighbors = self.y1.iloc[indices[0]]\n",
    "            CAT_0.append(y_neighbors.mean() - self.y0.loc[i])\n",
    "\n",
    "        # Calculate and return ATE\n",
    "        all_CAT = CAT_1 + CAT_0\n",
    "        self.ATE[k] = np.mean(all_CAT)\n",
    "        print(f\"ATE for k = {k} : {self.ATE[k]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409537b9",
   "metadata": {},
   "source": [
    "# Young, no children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7456eccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T20:18:01.460189Z",
     "start_time": "2024-09-15T20:18:01.448307Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_normalized (1594, 96)\n",
      "X_test_normalized (282, 96)\n",
      "Y_train (1594,)\n",
      "Y_test (282,)\n",
      "T_train (1594,)\n",
      "T_test (282,)\n"
     ]
    }
   ],
   "source": [
    "with open('./preprocessed_data/df_young_no_children_dict.pickle', 'rb') as f:\n",
    "    df_young_no_children_dict = pickle.load(f)\n",
    "\n",
    "for key in df_young_no_children_dict:\n",
    "    print(key, df_young_no_children_dict[key].shape)\n",
    "\n",
    "threshold = 2\n",
    "df_young_no_children_dict['T_train'] = df_young_no_children_dict['T_train'].apply(lambda x: 0 if x <= threshold else 1)\n",
    "df_young_no_children_dict['T_test'] = df_young_no_children_dict['T_test'].apply(lambda x: 0 if x <= threshold else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f414b23b9ee07db8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-15T20:18:03.476865Z",
     "start_time": "2024-09-15T20:18:03.466261Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       2\n",
       "1       2\n",
       "2       0\n",
       "3       2\n",
       "4       2\n",
       "       ..\n",
       "1589    4\n",
       "1590    0\n",
       "1591    3\n",
       "1592    4\n",
       "1593    5\n",
       "Name: # BIO CHILDREN REPORTED, Length: 1594, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_young_no_children_dict['Y_train']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144db651",
   "metadata": {},
   "source": [
    "### s-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fb0489d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "Fitted Linear Regression\n",
      "Fitted SVR with RBF kernel\n",
      "Fitted SVR with Polynomial kernel\n",
      "Fitted Gradient Boosting Regressor\n",
      "Fitted RandomForest Regressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Idan\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted MLP Regressor\n",
      "linear_regression: Train MSE = 1.7677, Test MSE = 1.9905\n",
      "svr_rbf: Train MSE = 1.4573, Test MSE = 2.0655\n",
      "svr_poly: Train MSE = 1.4735, Test MSE = 2.1275\n",
      "gradient_boosting: Train MSE = 1.4268, Test MSE = 1.9731\n",
      "random_forest: Train MSE = 0.2737, Test MSE = 2.0522\n",
      "mlp: Train MSE = 0.4563, Test MSE = 2.7417\n",
      "\n",
      "The ATE are  {'linear_regression': 0.090576171875, 'svr_rbf': 0.015168178918918928, 'svr_poly': 0.01129313118584496, 'gradient_boosting': 0.1367072313726818, 'random_forest': 0.0781449893390192, 'mlp': 0.11924092847124036}\n",
      "\n",
      "\n",
      "Final ATE: 0.07667346654353997\n"
     ]
    }
   ],
   "source": [
    "s_learner_young_no_children = S_learner(x_train=df_young_no_children_dict['X_train_normalized'], \n",
    "                      y_train=df_young_no_children_dict['Y_train'], \n",
    "                      x_test=df_young_no_children_dict['X_test_normalized'],\n",
    "                      y_test=df_young_no_children_dict['Y_test'],\n",
    "                      T_train=df_young_no_children_dict['T_train'],\n",
    "                      T_test=df_young_no_children_dict['T_test'])\n",
    "\n",
    "s_learner_young_no_children.fit()\n",
    "s_learner_young_no_children.evaluate()\n",
    "s_learner_young_no_children.compute_ATE()\n",
    "ATE = s_learner_young_no_children.compute_ATE_final(['svr_rbf', 'gradient_boosting', 'random_forest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c241894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "bootstrap 4/200\n",
      "bootstrap 40/200\n",
      "bootstrap 80/200\n",
      "bootstrap 120/200\n",
      "bootstrap 160/200\n",
      "mean_ATE 0.10576923636620464 CI_lower 0.01893205179875627 CI_upper 0.2194044827544131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.10576923636620464, 0.01893205179875627, 0.2194044827544131)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_learner_young_no_children = S_learner(x_train=df_young_no_children_dict['X_train_normalized'], \n",
    "                      y_train=df_young_no_children_dict['Y_train'], \n",
    "                      x_test=df_young_no_children_dict['X_test_normalized'],\n",
    "                      y_test=df_young_no_children_dict['Y_test'],\n",
    "                      T_train=df_young_no_children_dict['T_train'],\n",
    "                      T_test=df_young_no_children_dict['T_test'])\n",
    "s_learner_young_no_children.bootstrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab3e09b",
   "metadata": {},
   "source": [
    "### t-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed6e6541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "\n",
      "Fitting models with T = 0\n",
      "Fitted Linear Regression\n",
      "Fitted SVR with RBF kernel\n",
      "Fitted SVR with Polynomial kernel\n",
      "Fitted Gradient Boosting Regressor\n",
      "Fitted RandomForest Regressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Idan\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted MLP Regressor\n",
      "\n",
      "Fitting models with T = 1\n",
      "Fitted Linear Regression\n",
      "Fitted SVR with RBF kernel\n",
      "Fitted SVR with Polynomial kernel\n",
      "Fitted Gradient Boosting Regressor\n",
      "Fitted RandomForest Regressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Idan\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted MLP Regressor\n",
      "T: 0 | linear_regression: Train MSE = 1.5563, Test MSE = 2.1413\n",
      "T: 0 | svr_rbf: Train MSE = 1.2455, Test MSE = 2.2364\n",
      "T: 0 | svr_poly: Train MSE = 1.2289, Test MSE = 2.2756\n",
      "T: 0 | gradient_boosting: Train MSE = 1.1423, Test MSE = 2.2239\n",
      "T: 0 | random_forest: Train MSE = 0.2586, Test MSE = 2.1453\n",
      "T: 0 | mlp: Train MSE = 0.2972, Test MSE = 2.8307\n",
      "T: 1 | linear_regression: Train MSE = 1.9322, Test MSE = 2284827504130483442247794688.0000\n",
      "T: 1 | svr_rbf: Train MSE = 1.5923, Test MSE = 1.9011\n",
      "T: 1 | svr_poly: Train MSE = 1.5788, Test MSE = 1.9377\n",
      "T: 1 | gradient_boosting: Train MSE = 1.2511, Test MSE = 1.9226\n",
      "T: 1 | random_forest: Train MSE = 0.3479, Test MSE = 2.0357\n",
      "T: 1 | mlp: Train MSE = 0.5140, Test MSE = 2.1686\n",
      "\n",
      "The ATE are  {'linear_regression': 317550707762.75073, 'svr_rbf': 0.22836346192790138, 'svr_poly': 0.18112187272559666, 'gradient_boosting': 0.22528227528253794, 'random_forest': 0.2041950959488273, 'mlp': 0.23820328150758818}\n",
      "\n",
      "\n",
      "Final ATE: 0.2240110286667137\n"
     ]
    }
   ],
   "source": [
    "t_learner_df_young_no_children = T_learner(x_train=df_young_no_children_dict['X_train_normalized'], \n",
    "                      y_train=df_young_no_children_dict['Y_train'], \n",
    "                      x_test=df_young_no_children_dict['X_test_normalized'],\n",
    "                      y_test=df_young_no_children_dict['Y_test'],\n",
    "                      T_train=df_young_no_children_dict['T_train'],\n",
    "                      T_test=df_young_no_children_dict['T_test'])\n",
    "\n",
    "t_learner_df_young_no_children.fit()\n",
    "t_learner_df_young_no_children.evaluate()\n",
    "t_learner_df_young_no_children.compute_ATE()\n",
    "_ = t_learner_df_young_no_children.compute_ATE_final(['svr_rbf', 'gradient_boosting', 'random_forest'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "197c8055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "bootstrap 4/200\n",
      "bootstrap 40/200\n",
      "bootstrap 80/200\n",
      "bootstrap 120/200\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2820\\508835581.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m                       \u001b[0mT_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf_young_no_children_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'T_train'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m                       T_test=df_young_no_children_dict['T_test'])\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0ms_learner_young_no_children\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbootstrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2820\\428593079.py\u001b[0m in \u001b[0;36mbootstrap\u001b[1;34m(self, num_trials, alpha)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m             \u001b[1;31m# Step 3: Train the model on bootstrap sample\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train_boot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mT_train_boot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train_boot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbootstrap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m             \u001b[1;31m# Step 4: Compute ATE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2820\\428593079.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x_train, T_train, y_train, bootstrap)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'random_forest'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'random_forest'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train_full\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mbootstrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Fitted RandomForest Regressor\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    448\u001b[0m             \u001b[1;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m             \u001b[1;31m# since correctness does not rely on using threads.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 450\u001b[1;33m             trees = Parallel(\n\u001b[0m\u001b[0;32m    451\u001b[0m                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    452\u001b[0m                 \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1044\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1045\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1046\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1047\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    860\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 861\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    260\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    263\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"balanced\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m   1313\u001b[0m         \"\"\"\n\u001b[0;32m   1314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1315\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m   1316\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1317\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    418\u001b[0m             )\n\u001b[0;32m    419\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 420\u001b[1;33m         \u001b[0mbuilder\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "t_learner_df_young_no_children = T_learner(x_train=df_young_no_children_dict['X_train_normalized'], \n",
    "                      y_train=df_young_no_children_dict['Y_train'], \n",
    "                      x_test=df_young_no_children_dict['X_test_normalized'],\n",
    "                      y_test=df_young_no_children_dict['Y_test'],\n",
    "                      T_train=df_young_no_children_dict['T_train'],\n",
    "                      T_test=df_young_no_children_dict['T_test'])\n",
    "s_learner_young_no_children.bootstrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb976f30",
   "metadata": {},
   "source": [
    "### matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5cc6789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE for k = 1 : 0.20042643923240938\n",
      "ATE for k = 3 : 0.1931414356787491\n",
      "ATE for k = 5 : 0.18933901918976548\n",
      "ATE for k = 9 : 0.19189765458422173\n",
      "ATE for k = 15 : 0.18926794598436392\n",
      "ATE for k = 50 : 0.19841151385927508\n"
     ]
    }
   ],
   "source": [
    "matching_df_young_no_children = Matching(x=pd.concat([df_young_no_children_dict['X_train_normalized'].reset_index(drop=True), df_young_no_children_dict['X_test_normalized'].reset_index(drop=True)], axis=0),\n",
    "                     y=pd.concat([df_young_no_children_dict['Y_train'].reset_index(drop=True), df_young_no_children_dict['Y_test'].reset_index(drop=True)], axis=0),\n",
    "                     T=pd.concat([df_young_no_children_dict['T_train'], df_young_no_children_dict['T_test']], axis=0))\n",
    "\n",
    "matching_df_young_no_children.compute_ATE(1)\n",
    "matching_df_young_no_children.compute_ATE(3)\n",
    "matching_df_young_no_children.compute_ATE(5)\n",
    "matching_df_young_no_children.compute_ATE(9)\n",
    "matching_df_young_no_children.compute_ATE(15)\n",
    "matching_df_young_no_children.compute_ATE(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db81711",
   "metadata": {},
   "source": [
    "# Mature, no children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99aed23e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_normalized (1259, 92)\n",
      "X_test_normalized (223, 92)\n",
      "Y_train (1259,)\n",
      "Y_test (223,)\n",
      "T_train (1259,)\n",
      "T_test (223,)\n"
     ]
    }
   ],
   "source": [
    "with open('./preprocessed_data/df_mature_no_children_dict.pickle', 'rb') as f:\n",
    "    df_mature_no_children_dict = pickle.load(f)\n",
    "\n",
    "for key in df_young_no_children_dict:\n",
    "    print(key, df_mature_no_children_dict[key].shape)\n",
    "\n",
    "threshold = 2\n",
    "df_mature_no_children_dict['T_train'] = df_mature_no_children_dict['T_train'].apply(lambda x: 0 if x <= threshold else 1)\n",
    "df_mature_no_children_dict['T_test'] = df_mature_no_children_dict['T_test'].apply(lambda x: 0 if x <= threshold else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a2ea17",
   "metadata": {},
   "source": [
    "### s-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97982b3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "Fitted Linear Regression\n",
      "Fitted SVR with RBF kernel\n",
      "Fitted SVR with Polynomial kernel\n",
      "Fitted Gradient Boosting Regressor\n",
      "Fitted RandomForest Regressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Idan\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted MLP Regressor\n",
      "linear_regression: Train MSE = 1.3775, Test MSE = 3784026282075159527424.0000\n",
      "svr_rbf: Train MSE = 1.0752, Test MSE = 1.9891\n",
      "svr_poly: Train MSE = 1.0891, Test MSE = 1.9840\n",
      "gradient_boosting: Train MSE = 1.1355, Test MSE = 2.0120\n",
      "random_forest: Train MSE = 0.2248, Test MSE = 2.0275\n",
      "mlp: Train MSE = 0.2816, Test MSE = 2.8782\n",
      "\n",
      "The ATE are  {'linear_regression': 0.19810486869451657, 'svr_rbf': 0.09647427282194466, 'svr_poly': 0.004906583539126199, 'gradient_boosting': 0.12670466968455063, 'random_forest': 0.1166059379217274, 'mlp': 0.25578597685073307}\n",
      "\n",
      "\n",
      "Final ATE: 0.14889271431973894\n"
     ]
    }
   ],
   "source": [
    "s_learner_mature_no_children = S_learner(x_train=df_mature_no_children_dict['X_train_normalized'], \n",
    "                      y_train=df_mature_no_children_dict['Y_train'], \n",
    "                      x_test=df_mature_no_children_dict['X_test_normalized'],\n",
    "                      y_test=df_mature_no_children_dict['Y_test'],\n",
    "                      T_train=df_mature_no_children_dict['T_train'],\n",
    "                      T_test=df_mature_no_children_dict['T_test'])\n",
    "\n",
    "s_learner_mature_no_children.fit()\n",
    "s_learner_mature_no_children.evaluate()\n",
    "s_learner_mature_no_children.compute_ATE()\n",
    "_ = s_learner_mature_no_children.compute_ATE_final(['svr_rbf', 'gradient_boosting', 'random_forest', 'mlp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8629eb28",
   "metadata": {},
   "source": [
    "### t-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c12fdfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "\n",
      "Fitting models with T = 0\n",
      "Fitted Linear Regression\n",
      "Fitted SVR with RBF kernel\n",
      "Fitted SVR with Polynomial kernel\n",
      "Fitted Gradient Boosting Regressor\n",
      "Fitted RandomForest Regressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Idan\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted MLP Regressor\n",
      "\n",
      "Fitting models with T = 1\n",
      "Fitted Linear Regression\n",
      "Fitted SVR with RBF kernel\n",
      "Fitted SVR with Polynomial kernel\n",
      "Fitted Gradient Boosting Regressor\n",
      "Fitted RandomForest Regressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Idan\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted MLP Regressor\n",
      "T: 0 | linear_regression: Train MSE = 1.2871, Test MSE = 117131292249413271552.0000\n",
      "T: 0 | svr_rbf: Train MSE = 0.9825, Test MSE = 2.0614\n",
      "T: 0 | svr_poly: Train MSE = 0.9686, Test MSE = 1.9453\n",
      "T: 0 | gradient_boosting: Train MSE = 1.0099, Test MSE = 2.0870\n",
      "T: 0 | random_forest: Train MSE = 0.2272, Test MSE = 2.1192\n",
      "T: 0 | mlp: Train MSE = 0.2145, Test MSE = 2.5369\n",
      "T: 1 | linear_regression: Train MSE = 1.3091, Test MSE = 1143025717516736000497287168.0000\n",
      "T: 1 | svr_rbf: Train MSE = 1.0618, Test MSE = 1.8034\n",
      "T: 1 | svr_poly: Train MSE = 1.0137, Test MSE = 1.9750\n",
      "T: 1 | gradient_boosting: Train MSE = 0.8445, Test MSE = 2.0718\n",
      "T: 1 | random_forest: Train MSE = 0.2319, Test MSE = 1.8855\n",
      "T: 1 | mlp: Train MSE = 0.2814, Test MSE = 2.1397\n",
      "\n",
      "The ATE are  {'linear_regression': -2225627972584.482, 'svr_rbf': 0.2073146759927596, 'svr_poly': 0.21502515553069967, 'gradient_boosting': 0.20852074196226233, 'random_forest': 0.18539811066126854, 'mlp': 0.19106229166146788}\n",
      "\n",
      "\n",
      "Final ATE: 0.2240110286667137\n"
     ]
    }
   ],
   "source": [
    "t_learner_mature_no_children = T_learner(x_train=df_mature_no_children_dict['X_train_normalized'], \n",
    "                      y_train=df_mature_no_children_dict['Y_train'], \n",
    "                      x_test=df_mature_no_children_dict['X_test_normalized'],\n",
    "                      y_test=df_mature_no_children_dict['Y_test'],\n",
    "                      T_train=df_mature_no_children_dict['T_train'],\n",
    "                      T_test=df_mature_no_children_dict['T_test'])\n",
    "\n",
    "t_learner_mature_no_children.fit()\n",
    "t_learner_mature_no_children.evaluate()\n",
    "t_learner_mature_no_children.compute_ATE()\n",
    "_ = t_learner_df_young_no_children.compute_ATE_final(['svr_rbf', 'gradient_boosting', 'random_forest', 'mlp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78c98f8",
   "metadata": {},
   "source": [
    "### matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c8f6666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE for k = 1 : 0.2834008097165992\n",
      "ATE for k = 3 : 0.2595591542959964\n",
      "ATE for k = 5 : 0.2522267206477733\n",
      "ATE for k = 9 : 0.2419403208876893\n",
      "ATE for k = 15 : 0.24862798020692758\n",
      "ATE for k = 50 : 0.2540215924426451\n"
     ]
    }
   ],
   "source": [
    "matching_mature_no_children = Matching(x=pd.concat([df_mature_no_children_dict['X_train_normalized'].reset_index(drop=True), df_mature_no_children_dict['X_test_normalized'].reset_index(drop=True)], axis=0),\n",
    "                     y=pd.concat([df_mature_no_children_dict['Y_train'].reset_index(drop=True), df_mature_no_children_dict['Y_test'].reset_index(drop=True)], axis=0),\n",
    "                     T=pd.concat([df_mature_no_children_dict['T_train'], df_mature_no_children_dict['T_test']], axis=0))\n",
    "\n",
    "matching_mature_no_children.compute_ATE(1)\n",
    "matching_mature_no_children.compute_ATE(3)\n",
    "matching_mature_no_children.compute_ATE(5)\n",
    "matching_mature_no_children.compute_ATE(9)\n",
    "matching_mature_no_children.compute_ATE(15)\n",
    "matching_mature_no_children.compute_ATE(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7eea5c",
   "metadata": {},
   "source": [
    "# Mature, with children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09978027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_normalized (461, 81)\n",
      "X_test_normalized (82, 81)\n",
      "Y_train (461,)\n",
      "Y_test (82,)\n",
      "T_train (461,)\n",
      "T_test (82,)\n"
     ]
    }
   ],
   "source": [
    "with open('./preprocessed_data/df_mature_with_children_dict.pickle', 'rb') as f:\n",
    "    df_mature_with_children_dict = pickle.load(f)\n",
    "\n",
    "for key in df_mature_with_children_dict:\n",
    "    print(key, df_mature_with_children_dict[key].shape)\n",
    "\n",
    "threshold = 2\n",
    "df_mature_with_children_dict['T_train'] = df_mature_with_children_dict['T_train'].apply(lambda x: 0 if x <= threshold else 1)\n",
    "df_mature_with_children_dict['T_test'] = df_mature_with_children_dict['T_test'].apply(lambda x: 0 if x <= threshold else 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156a0c7",
   "metadata": {},
   "source": [
    "### s-learner "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8ba05f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "Fitted Linear Regression\n",
      "Fitted SVR with RBF kernel\n",
      "Fitted SVR with Polynomial kernel\n",
      "Fitted Gradient Boosting Regressor\n",
      "Fitted RandomForest Regressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Idan\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted MLP Regressor\n",
      "linear_regression: Train MSE = 1.2195, Test MSE = 83557686999417607627472896.0000\n",
      "svr_rbf: Train MSE = 0.9800, Test MSE = 2.5409\n",
      "svr_poly: Train MSE = 1.0299, Test MSE = 2.6754\n",
      "gradient_boosting: Train MSE = 0.6588, Test MSE = 2.6620\n",
      "random_forest: Train MSE = 0.2133, Test MSE = 2.8124\n",
      "mlp: Train MSE = 0.3502, Test MSE = 2.9392\n",
      "\n",
      "The ATE are  {'linear_regression': 0.22619831232734808, 'svr_rbf': 0.07309600183981839, 'svr_poly': 0.005901315788429757, 'gradient_boosting': 0.2288115604682558, 'random_forest': 0.13784530386740332, 'mlp': 0.16076499723706414}\n",
      "\n",
      "\n",
      "Final ATE: 0.1501294658531354\n"
     ]
    }
   ],
   "source": [
    "s_learner_mature_with_children = S_learner(x_train=df_mature_with_children_dict['X_train_normalized'], \n",
    "                      y_train=df_mature_with_children_dict['Y_train'], \n",
    "                      x_test=df_mature_with_children_dict['X_test_normalized'],\n",
    "                      y_test=df_mature_with_children_dict['Y_test'],\n",
    "                      T_train=df_mature_with_children_dict['T_train'],\n",
    "                      T_test=df_mature_with_children_dict['T_test'])\n",
    "\n",
    "s_learner_mature_with_children.fit()\n",
    "s_learner_mature_with_children.evaluate()\n",
    "s_learner_mature_with_children.compute_ATE()\n",
    "_ = s_learner_mature_with_children.compute_ATE_final(['svr_rbf', 'gradient_boosting', 'random_forest', 'mlp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970ec9e3",
   "metadata": {},
   "source": [
    "### t-learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5547cf39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "The DataFrame does not contain any null values.\n",
      "\n",
      "Fitting models with T = 0\n",
      "Fitted Linear Regression\n",
      "Fitted SVR with RBF kernel\n",
      "Fitted SVR with Polynomial kernel\n",
      "Fitted Gradient Boosting Regressor\n",
      "Fitted RandomForest Regressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Idan\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted MLP Regressor\n",
      "\n",
      "Fitting models with T = 1\n",
      "Fitted Linear Regression\n",
      "Fitted SVR with RBF kernel\n",
      "Fitted SVR with Polynomial kernel\n",
      "Fitted Gradient Boosting Regressor\n",
      "Fitted RandomForest Regressor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Idan\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:692: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted MLP Regressor\n",
      "T: 0 | linear_regression: Train MSE = 0.9094, Test MSE = 2382318347313553647255683072.0000\n",
      "T: 0 | svr_rbf: Train MSE = 0.7490, Test MSE = 2.6650\n",
      "T: 0 | svr_poly: Train MSE = 0.7333, Test MSE = 2.7301\n",
      "T: 0 | gradient_boosting: Train MSE = 0.3968, Test MSE = 2.5158\n",
      "T: 0 | random_forest: Train MSE = 0.1866, Test MSE = 2.8019\n",
      "T: 0 | mlp: Train MSE = 0.3395, Test MSE = 4.5577\n",
      "T: 1 | linear_regression: Train MSE = 1.0651, Test MSE = 2.2975\n",
      "T: 1 | svr_rbf: Train MSE = 1.0697, Test MSE = 1.7810\n",
      "T: 1 | svr_poly: Train MSE = 1.2325, Test MSE = 1.9825\n",
      "T: 1 | gradient_boosting: Train MSE = 0.4115, Test MSE = 2.5985\n",
      "T: 1 | random_forest: Train MSE = 0.2621, Test MSE = 2.5171\n",
      "T: 1 | mlp: Train MSE = 0.5966, Test MSE = 2.3558\n",
      "\n",
      "The ATE are  {'linear_regression': 2960814150145.167, 'svr_rbf': 0.5315385459887676, 'svr_poly': 0.5145290895514322, 'gradient_boosting': 0.3517826302502345, 'random_forest': 0.3702025782688766, 'mlp': 0.15264405382106808}\n",
      "\n",
      "\n",
      "Final ATE: 0.3515419520822367\n"
     ]
    }
   ],
   "source": [
    "t_learner_mature_with_children = T_learner(x_train=df_mature_with_children_dict['X_train_normalized'], \n",
    "                      y_train=df_mature_with_children_dict['Y_train'], \n",
    "                      x_test=df_mature_with_children_dict['X_test_normalized'],\n",
    "                      y_test=df_mature_with_children_dict['Y_test'],\n",
    "                      T_train=df_mature_with_children_dict['T_train'],\n",
    "                      T_test=df_mature_with_children_dict['T_test'])\n",
    "\n",
    "t_learner_mature_with_children.fit()\n",
    "t_learner_mature_with_children.evaluate()\n",
    "t_learner_mature_with_children.compute_ATE()\n",
    "_ = t_learner_mature_with_children.compute_ATE_final(['svr_rbf', 'gradient_boosting', 'random_forest', 'mlp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8567e51",
   "metadata": {},
   "source": [
    "### matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac0f526f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ATE for k = 1 : 0.47882136279926335\n",
      "ATE for k = 3 : 0.4739103744628606\n",
      "ATE for k = 5 : 0.4909760589318601\n",
      "ATE for k = 9 : 0.45528954368733376\n",
      "ATE for k = 15 : 0.44616329036218544\n",
      "ATE for k = 50 : 0.4661510128913444\n"
     ]
    }
   ],
   "source": [
    "matching_mature_with_children = Matching(x=pd.concat([df_mature_with_children_dict['X_train_normalized'].reset_index(drop=True), df_mature_with_children_dict['X_test_normalized'].reset_index(drop=True)], axis=0),\n",
    "                     y=pd.concat([df_mature_with_children_dict['Y_train'].reset_index(drop=True), df_mature_with_children_dict['Y_test'].reset_index(drop=True)], axis=0),\n",
    "                     T=pd.concat([df_mature_with_children_dict['T_train'], df_mature_with_children_dict['T_test']], axis=0))\n",
    "\n",
    "matching_mature_with_children.compute_ATE(1)\n",
    "matching_mature_with_children.compute_ATE(3)\n",
    "matching_mature_with_children.compute_ATE(5)\n",
    "matching_mature_with_children.compute_ATE(9)\n",
    "matching_mature_with_children.compute_ATE(15)\n",
    "matching_mature_with_children.compute_ATE(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a1b030",
   "metadata": {},
   "source": [
    "# Causal Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "21e86a92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: causal-curve in c:\\users\\idan\\anaconda3\\lib\\site-packages (1.0.6)\n",
      "Requirement already satisfied: numpy in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (1.26.4)\n",
      "Requirement already satisfied: progressbar2 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (4.5.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (1.1.0)\n",
      "Requirement already satisfied: numpydoc in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (1.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (1.4.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (1.0.2)\n",
      "Requirement already satisfied: pygam in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (0.9.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (1.11.4)\n",
      "Requirement already satisfied: six in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (1.16.0)\n",
      "Requirement already satisfied: black in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (22.6.0)\n",
      "Requirement already satisfied: python-utils in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (3.8.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (2.8.2)\n",
      "Requirement already satisfied: coverage in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (7.6.1)\n",
      "Requirement already satisfied: statsmodels in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (0.14.3)\n",
      "Requirement already satisfied: patsy in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (0.5.6)\n",
      "Requirement already satisfied: future in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (0.18.2)\n",
      "Requirement already satisfied: pytest in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (7.1.2)\n",
      "Requirement already satisfied: pytz in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (2022.1)\n",
      "Requirement already satisfied: sphinx-rtd-theme in c:\\users\\idan\\anaconda3\\lib\\site-packages (from causal-curve) (2.0.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.4.3 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from black->causal-curve) (0.4.3)\n",
      "Requirement already satisfied: tomli>=1.1.0 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from black->causal-curve) (2.0.1)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from black->causal-curve) (4.3.0)\n",
      "Requirement already satisfied: pathspec>=0.9.0 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from black->causal-curve) (0.9.0)\n",
      "Requirement already satisfied: platformdirs>=2 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from black->causal-curve) (2.5.2)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from black->causal-curve) (8.0.4)\n",
      "Requirement already satisfied: Jinja2>=2.10 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from numpydoc->causal-curve) (2.11.3)\n",
      "Requirement already satisfied: sphinx>=3.0 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from numpydoc->causal-curve) (5.0.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from pytest->causal-curve) (21.4.0)\n",
      "Requirement already satisfied: iniconfig in c:\\users\\idan\\anaconda3\\lib\\site-packages (from pytest->causal-curve) (1.1.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\idan\\anaconda3\\lib\\site-packages (from pytest->causal-curve) (21.3)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from pytest->causal-curve) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from pytest->causal-curve) (1.11.0)\n",
      "Requirement already satisfied: atomicwrites>=1.0 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from pytest->causal-curve) (1.4.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\idan\\anaconda3\\lib\\site-packages (from pytest->causal-curve) (0.4.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from scikit-learn->causal-curve) (2.2.0)\n",
      "Requirement already satisfied: sphinxcontrib-jquery<5,>=4 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx-rtd-theme->causal-curve) (4.1)\n",
      "Requirement already satisfied: docutils<0.21 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx-rtd-theme->causal-curve) (0.18.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from Jinja2>=2.10->numpydoc->causal-curve) (2.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from packaging->pytest->causal-curve) (3.0.9)\n",
      "Requirement already satisfied: sphinxcontrib-jsmath in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx>=3.0->numpydoc->causal-curve) (1.0.1)\n",
      "Requirement already satisfied: requests>=2.5.0 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx>=3.0->numpydoc->causal-curve) (2.28.1)\n",
      "Requirement already satisfied: alabaster<0.8,>=0.7 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx>=3.0->numpydoc->causal-curve) (0.7.12)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx>=3.0->numpydoc->causal-curve) (4.11.3)\n",
      "Requirement already satisfied: sphinxcontrib-applehelp in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx>=3.0->numpydoc->causal-curve) (1.0.2)\n",
      "Requirement already satisfied: imagesize in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx>=3.0->numpydoc->causal-curve) (1.4.1)\n",
      "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx>=3.0->numpydoc->causal-curve) (2.0.0)\n",
      "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx>=3.0->numpydoc->causal-curve) (1.1.5)\n",
      "Requirement already satisfied: sphinxcontrib-devhelp in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx>=3.0->numpydoc->causal-curve) (1.0.2)\n",
      "Requirement already satisfied: Pygments>=2.0 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx>=3.0->numpydoc->causal-curve) (2.11.2)\n",
      "Requirement already satisfied: snowballstemmer>=1.1 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx>=3.0->numpydoc->causal-curve) (2.2.0)\n",
      "Requirement already satisfied: sphinxcontrib-qthelp in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx>=3.0->numpydoc->causal-curve) (1.0.3)\n",
      "Requirement already satisfied: babel>=1.3 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from sphinx>=3.0->numpydoc->causal-curve) (2.9.1)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from importlib-metadata>=4.4->sphinx>=3.0->numpydoc->causal-curve) (3.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from requests>=2.5.0->sphinx>=3.0->numpydoc->causal-curve) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from requests>=2.5.0->sphinx>=3.0->numpydoc->causal-curve) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from requests>=2.5.0->sphinx>=3.0->numpydoc->causal-curve) (1.26.11)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\idan\\anaconda3\\lib\\site-packages (from requests>=2.5.0->sphinx>=3.0->numpydoc->causal-curve) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install causal-curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf6a394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "from causal_curve import GPS_Regressor\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bd4493",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mean_and_CI(ax, treatment, mean, lb, ub, color_mean=None, color_shading=None):\n",
    "    # Plot the shaded range of the confidence intervals\n",
    "    ax.fill_between(treatment, lb, ub, color=color_shading, alpha=0.3)\n",
    "    # Plot the mean on top\n",
    "    ax.plot(treatment, mean, color=color_mean, linewidth=0.75)\n",
    "\n",
    "def plot_causal_curve(results_dict, fig_name, title, x_label, y_label):\n",
    "    # Set plotting parameters\n",
    "    plt.rcParams['figure.dpi'] = 200\n",
    "    plt.rcParams['figure.figsize'] = [6, 5]\n",
    "\n",
    "    # Create a single figure and axis\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Plotting quantities\n",
    "    treat = results_dict['Treatment']\n",
    "    mean = results_dict['Causal_Dose_Response']\n",
    "    lb = results_dict['Lower_CI']\n",
    "    ub = results_dict['Upper_CI']\n",
    "\n",
    "    # Plot the data\n",
    "    plot_mean_and_CI(ax, treat, mean, lb, ub, color_mean='b', color_shading='b')\n",
    "\n",
    "    # Labels\n",
    "    ax.tick_params(axis='both', which='major', labelsize=10)\n",
    "    ax.set_ylabel(y_label, fontsize=10)\n",
    "    ax.set_xlabel(x_label, fontsize=10)\n",
    "    ax.set_title(title, fontsize=12)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Add the main title\n",
    "    #fig.suptitle(title, fontsize=10)\n",
    "\n",
    "    # Customize plot appearance\n",
    "    ax.spines[\"top\"].set_visible(False)\n",
    "    ax.spines[\"right\"].set_visible(False)\n",
    "\n",
    "    # Calculate x-axis range with 5% padding\n",
    "    x_min, x_max = min(treat), max(treat)\n",
    "    x_range = x_max - x_min\n",
    "    x_padding = 0.05 * x_range\n",
    "    ax.set_xlim(x_min - x_padding, x_max + x_padding)\n",
    "\n",
    "    # Calculate y-axis range based on lb and ub with 5% padding\n",
    "    y_min, y_max = min(lb), max(ub)\n",
    "    y_range = y_max - y_min\n",
    "    y_padding = 0.05 * y_range\n",
    "    ax.set_ylim(y_min - y_padding, y_max + y_padding)\n",
    "\n",
    "    ax.tick_params(axis='both', which='major', labelsize=6)\n",
    "\n",
    "    # Adjust layout to accommodate titles\n",
    "    fig.tight_layout(rect=[0, 0.03, 1, 0.90])\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(f'{fig_name}_causal_curve.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8901b14",
   "metadata": {},
   "source": [
    "## Young, no children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "326182a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_normalized (1594, 96)\n",
      "X_test_normalized (282, 96)\n",
      "Y_train (1594,)\n",
      "Y_test (282,)\n",
      "T_train (1594,)\n",
      "T_test (282,)\n"
     ]
    }
   ],
   "source": [
    "with open('./preprocessed_data/df_young_no_children_dict.pickle', 'rb') as f:\n",
    "    df_young_no_children_dict = pickle.load(f)\n",
    "\n",
    "for key in df_young_no_children_dict:\n",
    "    print(key, df_young_no_children_dict[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd6bbfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_young_no_children_dict\n",
    "with open('df_young_no_children_dict.pickle', 'rb') as f:\n",
    "    df_young_no_children_dict = pickle.load(f)\n",
    "\n",
    "gps = GPS_Regressor()\n",
    "gps.fit(T=pd.concat([df_young_no_children_dict['T_train'], df_young_no_children_dict['T_test']], axis=0),\n",
    "        X=pd.concat([df_young_no_children_dict['X_train_normalized'].reset_index(drop=True), df_young_no_children_dict['X_test_normalized'].reset_index(drop=True)], axis=0),\n",
    "        y=pd.concat([df_young_no_children_dict['Y_train'].reset_index(drop=True), df_young_no_children_dict['Y_test'].reset_index(drop=True)], axis=0).astype('Float64'))\n",
    "gps_results = gps.calculate_CDRC(0.95)\n",
    "plot_causal_curve(gps_results, fig_name=\"df_young_no_children_dict\",\n",
    "                  title=\"Causal Dose-Response Curve (with 95% CI)\\nYoung without children participants\",\n",
    "                  x_label=\"Num. of children expected\", y_label=\"Num. of children\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1438a",
   "metadata": {},
   "source": [
    "## Mature, no children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21876a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('df_mature_no_children_dict.pickle', 'rb') as f:\n",
    "    df_mature_no_children_dict = pickle.load(f)\n",
    "\n",
    "gps = GPS_Regressor()\n",
    "gps.fit(T=pd.concat([df_mature_no_children_dict['T_train'], df_mature_no_children_dict['T_test']], axis=0),\n",
    "        X=pd.concat([df_mature_no_children_dict['X_train_normalized'].reset_index(drop=True), df_mature_no_children_dict['X_test_normalized'].reset_index(drop=True)], axis=0),\n",
    "        y=pd.concat([df_mature_no_children_dict['Y_train'].reset_index(drop=True), df_mature_no_children_dict['Y_test'].reset_index(drop=True)], axis=0).astype('Float64'))\n",
    "gps_results = gps.calculate_CDRC(0.95)\n",
    "plot_causal_curve(gps_results, fig_name=\"df_mature_no_children_dict\",\n",
    "                  title=\"Causal Dose-Response Curve (with 95% CI)\\nMature without children participants\",\n",
    "                  x_label=\"Num. of children expected\", y_label=\"Num. of children\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d17f65",
   "metadata": {},
   "source": [
    "## Mature, with children"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fd7781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mature_with_children_dict\n",
    "with open('df_mature_with_children_dict.pickle', 'rb') as f:\n",
    "    df_mature_with_children_dict = pickle.load(f)\n",
    "\n",
    "# delete features with low variance\n",
    "df_mature_with_children_dict['X_train_normalized'] = df_mature_with_children_dict['X_train_normalized'].drop(\n",
    "    \"SAMPLE ID  79 INT_MIL FEMALE HISPANIC\", axis=1)\n",
    "df_mature_with_children_dict['X_test_normalized'] = df_mature_with_children_dict['X_test_normalized'].drop(\n",
    "    \"SAMPLE ID  79 INT_MIL FEMALE HISPANIC\", axis=1)\n",
    "\n",
    "# For the training data\n",
    "# Create a boolean mask where T_train <= 10\n",
    "mask_train = df_mature_with_children_dict['T_train'] <= 11\n",
    "\n",
    "# Apply the mask to T_train, X_train_normalized, and Y_train\n",
    "df_mature_with_children_dict['T_train'] = df_mature_with_children_dict['T_train'][mask_train]\n",
    "df_mature_with_children_dict['X_train_normalized'] = df_mature_with_children_dict['X_train_normalized'][mask_train]\n",
    "df_mature_with_children_dict['Y_train'] = df_mature_with_children_dict['Y_train'][mask_train]\n",
    "\n",
    "# For the test data\n",
    "# Create a boolean mask where T_test <= 10\n",
    "mask_test = df_mature_with_children_dict['T_test'] <= 11\n",
    "\n",
    "# Apply the mask to T_test, X_test_normalized, and Y_test\n",
    "df_mature_with_children_dict['T_test'] = df_mature_with_children_dict['T_test'][mask_test]\n",
    "df_mature_with_children_dict['X_test_normalized'] = df_mature_with_children_dict['X_test_normalized'][mask_test]\n",
    "df_mature_with_children_dict['Y_test'] = df_mature_with_children_dict['Y_test'][mask_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee001fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps = GPS_Regressor()\n",
    "gps.fit(T=pd.concat([df_mature_with_children_dict['T_train'], df_mature_with_children_dict['T_test']], axis=0),\n",
    "        X=pd.concat([df_mature_with_children_dict['X_train_normalized'].reset_index(drop=True), df_mature_with_children_dict['X_test_normalized'].reset_index(drop=True)], axis=0),\n",
    "        y=pd.concat([df_mature_with_children_dict['Y_train'].reset_index(drop=True), df_mature_with_children_dict['Y_test'].reset_index(drop=True)], axis=0).astype('Float64'))\n",
    "gps_results = gps.calculate_CDRC(0.95)\n",
    "plot_causal_curve(gps_results, fig_name=\"df_mature_with_children_dict\",\n",
    "                  title=\"Causal Dose-Response Curve (with 95% CI)\\nMature with children participants\",\n",
    "                  x_label=\"Num. of children expected\", y_label=\"Num. of children\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
